# Gaussian-Gated IndRNN と逆Forward-Forward学習

本ドキュメントでは，差分ベースのGaussianゲートを備えたIndRNNモデルと，
逆Forward-Forward勾配およびエネルギー保存スケーリングを組み合わせた
学習則について数式を中心にまとめる。

## 1. モデル構造

### 1.1 CNNエンコーダ
入力フレーム列 \(\{X_t\}\) を深層CNNで処理し，特徴ベクトル
\(f_t = \mathrm{CNN}(X_t)\) を得る。

### 1.2 フレーム差分の正規化
隣接特徴の差分 \(d_t = f_t - f_{t-1}\) を計算し，各要素で
\[
  \tilde d_{t,i} = \frac{d_{t,i}}{\varepsilon + |d_{t,i}|}
\]
と正規化する (\(\varepsilon\) は小正数)。

### 1.3 Gaussianゲート生成
正規化差分に基づきゲート
\[
  g_{t,i} = \exp\Bigl(-\frac{\tilde d_{t,i}^2}{2\sigma^2}\Bigr), \quad 0 < g_{t,i} \le 1
\]
を求める。\(\sigma\) は学習可能なスケールである。

### 1.4 IndRNN更新
各ユニットの再帰重みを \(w_i\)，入力重みを \(U_i\) とし，
活性化関数 \(\phi\) を用いて
\[
  h_{t,i} = \phi\bigl(\,w_i\,g_{t,i} h_{t-1,i} + U_i^{\top} x_t + b_i\bigr)
\]
と更新する。ここで \(x_t = f_t\) など入力特徴を表す。

## 2. 逆Forward-Forward勾配

正例パス \(h_l^+\) と負例パス \(h_l^-\) を用意し，局所目的
\[
  J_l = \tfrac12\bigl(\|h_l^+\|^2 - \|h_l^-\|^2\bigr)
\]
を最大化する。線形変換層の重み更新は
\[
  \Delta w_{ij} = \eta\bigl(h_{l,i}^+ x_{l,j}^+ - h_{l,i}^- x_{l,j}^-\bigr)
\]
となる。ここで \(\eta\) は学習率である。

## 3. エネルギー保存スケーリング

重み行列 \(W\) の Frobenius ノルムを一定に保つ近似として，更新
\(\Delta W\) にスケーリング係数
\[
  \alpha = -\frac{\langle W, \Delta W \rangle}{\|\Delta W\|^2 + \epsilon}
\]
を掛ける。バイアスについても同様に処理する。\(\epsilon\) は除算防止項。

## 4. 学習手順概要
1. 正例データと負例データを用意し，各々 CNN–IndRNN へ順伝播させ
   \(h_l^+, h_l^-\) を得る。
2. 局所勾配 \(\Delta W\) を計算し，エネルギー保存スケーリング
   \(\alpha\Delta W\) を重みに加える。
3. 以上を全層で行うことで，バックプロパゲーションを用いずに
   ネットワークが訓練される。

## 5. 数理的考察
Gaussianゲートにより時系列差分の大きさに応じて再帰成分の寄与が
変調される。小さな変化では過去の状態を保持し，大きな変化では
新規入力が優先されるため，長期依存学習と変動検知を両立できる。
逆Forward-Forward則は各層の出力エネルギー差を用いるため，
生物学的な局所学習則への接続が期待できる。さらにエネルギー保存
によるスケーリングは，重みノルムの発散を抑制して学習を安定化
させる役割を果たす。

